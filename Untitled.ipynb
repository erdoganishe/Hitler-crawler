{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52db473f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] input_json\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khoty\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "import argparse\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "\n",
    "def find_shortest_path(start, end):\n",
    "   \n",
    "    path = {}\n",
    "    path[start] = [start]\n",
    "    Q = deque([start])\n",
    "\n",
    "    while len(Q) != 0:\n",
    "       \n",
    "        page = Q.popleft()\n",
    "        links = get_links(page)\n",
    "\n",
    "       \n",
    "        for link in links:\n",
    "\n",
    "           \n",
    "            if link in end:\n",
    "                return path[page] + [link]\n",
    "\n",
    "         \n",
    "            if (link not in path) and (link != page):\n",
    "                path[link] = path[page] + [link]\n",
    "                Q.append(link)\n",
    "\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_links(page):\n",
    "\n",
    "    r = requests.get(page)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    base_url = page[:page.find('/wiki/')]\n",
    "    links = list({base_url + a['href'] for a in soup.select('p a[href]') if a['href'].startswith('/wiki/')})\n",
    "    return links\n",
    "\n",
    "def check_pages(start, end):\n",
    "    \n",
    "    languages = []\n",
    "    for page in [start, end]:\n",
    "        try:\n",
    "            ind = page.find('.wikipedia.org/wiki/')\n",
    "            languages.append(page[(ind-2):ind])\n",
    "            requests.get(page)\n",
    "        except:\n",
    "            print('{} does not appear to be a valid Wikipedia page.'.format(page))\n",
    "            return False\n",
    "\n",
    "    if len(set(languages)) > 1:\n",
    "        print('Pages are in different languages.')\n",
    "        return False\n",
    "\n",
    "    if len(get_links(start)) == 0:\n",
    "        print('Start page is a dead-end page with no Wikipedia links.')\n",
    "        return False\n",
    "\n",
    "    end_soup = BeautifulSoup(requests.get(end).content, 'html.parser')\n",
    "    if end_soup.find('table', {'class': 'metadata plainlinks ambox ambox-style ambox-Orphan'}):\n",
    "        print('End page is an orphan page with no Wikipedia pages linking to it.')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def redirected(end):\n",
    "\n",
    "    end_soup = BeautifulSoup(requests.get(end).content, 'html.parser')\n",
    "    title = end_soup.find('h1').text\n",
    "    title = title.replace(' ', '_', len(title))\n",
    "    base_url = end[:end.find('/wiki/') + len('/wiki/')]\n",
    "    return set([end, base_url + title])\n",
    "\n",
    "def result(start, end, path):\n",
    "\n",
    "    if path:\n",
    "        result = path\n",
    "    else:\n",
    "        result = \"No path! :( \"\n",
    "    d = {\"start\": start, \"end\": end, \"path\": result}\n",
    "    return json.dumps(d, indent=4)\n",
    "\n",
    "def main(args):\n",
    "   \n",
    "    input_json = args.input_json\n",
    "    data = json.loads(args.input_json)\n",
    "    start = data[\"start\"]\n",
    "    end = data[\"end\"]\n",
    "    if check_pages(start, end):\n",
    "        path = find_shortest_path(start, redirected(end))\n",
    "        json_result = result(start, end, path)\n",
    "        return json_result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    starttime = time.time()\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='WikiRacer for finding the shortest path between two Wikipedia articles')\n",
    "    parser.add_argument('input_json', help='JSON object with \"start\" & \"end\" name/value pairs of Wikipedia links')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(main(args))\n",
    "\n",
    "    endtime = time.time()\n",
    "    totaltime = endtime - starttime\n",
    "    print('Time: {}m {:.3f}s'.format(int(totaltime)/60, totaltime%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0a9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
